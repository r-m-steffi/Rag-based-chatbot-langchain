{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5592bf-b323-4ae8-8186-d85a7dd2422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96b5833-ffb4-4daa-a817-6ceb931f47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdf file\n",
    "pdf_path = \"/home/steffi/aisd/Books/Fundamentals of Machine Learning for Predictive Data Analytics 2015.pdf\" \n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a60310ac-5a23-4ec5-a4d7-3ca73e86dabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunck the document\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec16225-88a3-42a7-bfd9-8a17d463fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings using HuggingFace\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad360627-f8b4-46c8-8f54-37dbf840f515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create vectorstore\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc81bb91-0167-4c10-9d4e-e0a75204ec6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8af4b017-0b95-49ea-b3e3-9bcdabf2e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# 6. Load small local LLM using transformers pipeline (causal LM)\n",
    "llm_pipeline = pipeline(\"text-generation\", model=\"distilgpt2\", max_new_tokens=150)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37bda46-a922-4583-b7bb-84923d2837a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define a simple generation function using the pipeline\n",
    "def local_llm_generate(prompt):\n",
    "    outputs = llm_pipeline(prompt, max_length=150, do_sample=True, top_p=0.95, num_return_sequences=1)\n",
    "    return outputs[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4077ed02-1a82-4262-989f-3563a7c1ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Simple retrieval-augmented generation function\n",
    "def rag_query(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    return local_llm_generate(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70ddf8dd-4d42-42f2-b50e-4f9342a97fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=150) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is gradient descent?\n",
      "A: Context:\n",
      "7.6\tFurther\tReading\n",
      "The\tkey\tcomponent\tof\tthe\tgradient\tdescent\talgorithm\tpresented\tin\tthis\tchapter\tis\tthe\tuse\n",
      "of\t\n",
      "differentiation\n",
      "\tto\tcompute\tthe\tslope\tof\tthe\terror\tsurface.\tDifferentiation\tis\ta\tpart\tof\n",
      "calculus,\twhich\tis\ta\tlarge\tand\tvery\timportant\tfield\tof\tmathematics.\tIn\tAppendix\tC\n",
      "[551]\n",
      "\twe\n",
      "provide\tan\tintroduction\tto\tdifferentiation\tthat\tcovers\tall\tthe\ttechniques\trequired\tto\n",
      "understand\thow\tthe\tgradient\tdescent\talgorithm\tworks.\tIf,\thowever,\tyou\twish\tto\tget\ta\n",
      "gradient\tdescent\n",
      ".\n",
      "To\tunderstand\thow\tgradient\tdescent\tworks,\timagine\ta\thiker\tunlucky\tenough\tto\tbe\n",
      "stranded\ton\tthe\tside\tof\ta\tvalley\ton\ta\tfoggy\tday.\tBecause\tof\tthe\tdense\tfog,\tit\tis\tnot\tpossible\n",
      "for\ther\tto\tsee\tthe\tway\tto\ther\tdestination\tat\tthe\tbottom\tof\tthe\tvalley.\tInstead,\tit\tis\tonly\n",
      "possible\tto\tsee\tthe\tground\tat\ther\tfeet\tto\twithin\tabout\ta\tthree\tfoot\tradius.\tIt\tmight,\tat\tfirst,\n",
      "seem\tlike\tall\tis\tlost\tand\tthat\tit\twill\tbe\timpossible\tfor\tthe\thiker\tto\tfind\ther\tway\tdown\tto\tthe\n",
      "accurate\tpredictive\tmodel\tpossible.\n",
      "The\tmost\timportant\tpart\tto\tthe\tgradient\tdescent\talgorithm\tis\tthe\tline\ton\twhich\tthe\n",
      "weights\tare\tupdated,\tLine\t4\n",
      "[339]\n",
      ".\tEach\tweight\tis\tconsidered\tindependently,\tand\tfor\teach\n",
      "one\ta\tsmall\tadjustment\tis\tmade\tby\tadding\ta\tsmall\tvalue,\tcalled\ta\t\n",
      "delta\tvalue\n",
      ",\tto\tthe\n",
      "current\tweight,\t\n",
      "w\n",
      "[\n",
      "j\n",
      "].\tThis\tadjustment\tshould\tensure\tthat\tthe\tchange\tin\tthe\tweight\tleads\tto\n",
      "a\tmove\t\n",
      "downward\n",
      "\ton\tthe\terror\tsurface.\t\n",
      "The\tlearning\trate,\t\n",
      "α\n",
      ",\tdetermines\tthe\tsize\tof\tthe\n",
      "Figure\t7.4\n",
      "(a)\tA\t3D\tplot\tof\tan\terror\tsurface\tand\t(b)\ta\tbird’s-eye\tview\tcontour\tplot\tof\tthe\tsame\terror\n",
      "surface.\tThe\tlines\tindicate\tthe\tpath\tthat\tthe\tgradient\tdescent\talgorithm\twould\ttake\tacross\n",
      "this\terror\tsurface\tfrom\t4\tdifferent\tstarting\tpositions\tto\tthe\tglobal\tminimum—marked\tas\n",
      "the\twhite\tdot\tin\tthe\tcenter.\n",
      "The\tgradient\tdescent\talgorithm\tfor\ttraining\tmultivariable\tregression\tmodels\tis\tformally\n",
      "presented\tin\t\n",
      "Algorithm\t7.1\n",
      "[339]\n",
      ".\tEach\tweight\tis\titeratively\tadjusted\tby\ta\tsmall\tamount\n",
      "\n",
      "Question: What is gradient descent?\n",
      "Answer:\n",
      "[552]\n",
      "[553]\n",
      "[554]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[555]\n",
      "[\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Ask a question\n",
    "question = \"What is gradient descent?\"\n",
    "answer = rag_query(question)\n",
    "\n",
    "print(f\"Q: {question}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec02c90d-ad18-42b8-b8ad-2ebe0d7b673f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
